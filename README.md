# Hanhan-TravelPlusPlus
Using machine learning, data mining, data visualization techniques

* Project Presentation Slides - travel++ slides.pdf

* Using Spark Cloud
  * The code is written and tested in Spark Cluster (Cloud). Spark Cluster is super awesome! It has Scala, Python, R, Spark SQL Notebook and each Notebook will work for all these languages in a very convenient and fast way! Each day I just need to open my browser and work on the cluster. The Notebook just functions like IPython Notebook, each cell will show output. It is also very fast to attach libraries to the cluster. There are much more advantages about using Spark Cluster, I will write them and what to note in the following implemenation detials.

* Main Features
  * Gossip Queen - Real Time Hottest Trends world wide
    1. World Wide Hottest Trends in different countries - The hot trends in this part may not be real time, the origional data generated from Twitter trends API, I extracted the hashtag and the url from the data. The trends got here may not be real time but still the newest within 2 days! But you may note that, here I am using limited amount of countries to extract the data and I am not using Spark RDD or DataFrame to do parallized requests. Because of the limitation of Twitter trends api. If I am using parallized requests or do more country trend request, Trwitter will send 429 HTTP error which means too many requests. But by doing the intersection of world-wide trends with each country trends, the trends showing here are newest popular country trends world-wide! This is the just the appetizer I am showing :)
    2. Real Time Trends - The data source is twitter search API with "travel" as the search key words. I have cleaned the data and analysis based on words frequency. The words were extractes from hashtags, tweet text and user mentions (screen names). Finally, tweets are sort by retweet count in descending order. Origionally, I was using Python pretty table, but it bacame pretty slow when there were more tweets to deal with. However, using Spark DataFrame became much faster, it will easier to do sorting, or other sql query in the future.
    3. Hot tourism sports - This is my favorite part in Gossip Queen. It is using a new way to recommend hot tourism spots instead of using stored tourist spots, this will present the real trends at current time. For example, several people has visited the same town in California, but that maybe the Ghost Town, or China Town, or Vingine Lake. These 3 palces all share similar latitude and longtitude. People travel to the same town for different purpose, this is very interesting. By doing frequency analysis, I can do tourism spots recommendation. The detailed implementation looks tricky but interesting, sicne it overcomes the shortage of different APIs and in fact takes advantage of these shortages :)

       Here's how am I handling this intresting problem: The Flickr photo API will return photo ids by giving a search key word like "travel", by using the ids, the latitude and the longtide of the photo can also be found. But you cannot get any relative tags similar to "travel". The Instagram API will return relative tages by just giving "travel" as the key word, but the API is funnny that it needs you to give latitude and the longtitude when search for photos... However, once you give it the latitude and the longtitude, it returns the photos posted in that area with detailed location info. Just like my example, Gost Town or China Town or Vingine Lake in that area. But most of the location names do not indicate the city/State/country, so I am using Google api here to get the city, State and the country. But Google API has request limitation, once I have reached to the rate limit, it retuns me null.

!!Note: I am using DataFame and udf for the 2nd and the 3rd parts of Gossip Queen, since it makes the calculation mush faster because of parallized calculation and Spark awesome architecture. However, it is better to cache() the generated dataframe if you need to use it later, otherwise running the code in Spark python Notebook using different cells, you may get "lost task" error.
And in many cases, it is really good to run code in different cells, since it stores some previous generated output which will be used later. You don't need to re-run previous code again, which saves much time. But if you have dettached your Cluster, you have to re-run the code from the begining. And click "Run All" is not a good choice since Spark will run all the cells together in stead of running from the top to the bottom.
